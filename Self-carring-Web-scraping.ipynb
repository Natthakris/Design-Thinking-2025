{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNg6T+RrsePCtgxK6C3QvuD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Natthakris/Design-Thinking-2025/blob/main/Self-carring-Web-scraping.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AcdEj6rBEs87",
        "outputId": "11952080-d69e-469d-b9c0-373b9de4e73d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "กำลังดึงข้อมูลจาก URL: https://multimedia.anamai.moph.go.th/help-knowledgs/\n",
            "กำลังดึงข้อมูลจาก URL: https://multimedia.anamai.moph.go.th/help-knowledgs/page/2/\n",
            "กำลังดึงข้อมูลจาก URL: https://multimedia.anamai.moph.go.th/help-knowledgs/page/3/\n",
            "กำลังดึงข้อมูลจาก URL: https://multimedia.anamai.moph.go.th/help-knowledgs/page/4/\n",
            "กำลังดึงข้อมูลจาก URL: https://multimedia.anamai.moph.go.th/help-knowledgs/page/5/\n",
            "กำลังดึงข้อมูลจาก URL: https://multimedia.anamai.moph.go.th/help-knowledgs/page/6/\n",
            "กำลังดึงข้อมูลจาก URL: https://multimedia.anamai.moph.go.th/help-knowledgs/page/7/\n",
            "กำลังดึงข้อมูลจาก URL: https://multimedia.anamai.moph.go.th/help-knowledgs/page/8/\n",
            "กำลังดึงข้อมูลจาก URL: https://multimedia.anamai.moph.go.th/help-knowledgs/page/9/\n",
            "กำลังดึงข้อมูลจาก URL: https://multimedia.anamai.moph.go.th/help-knowledgs/page/10/\n",
            "กำลังดึงข้อมูลจาก URL: https://multimedia.anamai.moph.go.th/help-knowledgs/page/11/\n",
            "กำลังดึงข้อมูลจาก URL: https://multimedia.anamai.moph.go.th/help-knowledgs/page/12/\n",
            "กำลังดึงข้อมูลจาก URL: https://multimedia.anamai.moph.go.th/help-knowledgs/page/13/\n",
            "กำลังดึงข้อมูลจาก URL: https://multimedia.anamai.moph.go.th/help-knowledgs/page/14/\n",
            "กำลังดึงข้อมูลจาก URL: https://multimedia.anamai.moph.go.th/help-knowledgs/page/15/\n",
            "กำลังดึงข้อมูลจาก URL: https://multimedia.anamai.moph.go.th/help-knowledgs/page/16/\n",
            "กำลังดึงข้อมูลจาก URL: https://multimedia.anamai.moph.go.th/help-knowledgs/page/17/\n",
            "กำลังดึงข้อมูลจาก URL: https://multimedia.anamai.moph.go.th/help-knowledgs/page/18/\n",
            "กำลังดึงข้อมูลจาก URL: https://multimedia.anamai.moph.go.th/help-knowledgs/page/19/\n",
            "กำลังดึงข้อมูลจาก URL: https://multimedia.anamai.moph.go.th/help-knowledgs/page/20/\n",
            "ดึงข้อมูลเสร็จสิ้นจาก 20 หน้า\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "\n",
        "# URL ของเว็บไซต์ที่ต้องการดึงข้อมูล\n",
        "base_url = \"https://multimedia.anamai.moph.go.th/help-knowledgs/\"\n",
        "all_scraped_content_strings = []\n",
        "page_num = 1\n",
        "max_pages_to_scrape = 20 # กำหนดจำนวนหน้าสูงสุดที่ต้องการดึงข้อมูล\n",
        "\n",
        "while page_num <= max_pages_to_scrape:\n",
        "    # สร้าง URL สำหรับหน้าปัจจุบัน\n",
        "    current_page_url = f\"{base_url}page/{page_num}/\" if page_num > 1 else base_url\n",
        "    print(f\"กำลังดึงข้อมูลจาก URL: {current_page_url}\")\n",
        "\n",
        "    try:\n",
        "        page = requests.get(current_page_url)\n",
        "        page.raise_for_status() # ตรวจสอบข้อผิดพลาด HTTP\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"เกิดข้อผิดพลาดในการดึงหน้า {current_page_url}: {e}\")\n",
        "        break # หยุดการทำงานหากมีข้อผิดพลาดในการร้องขอ\n",
        "\n",
        "    soup = BeautifulSoup(page.content, \"html.parser\")\n",
        "    current_page_content_elements = soup.find_all(class_=\"col-12 col-md-4 col-lg-4\")\n",
        "\n",
        "    if not current_page_content_elements:\n",
        "        print(f\"ไม่พบเนื้อหาหรือถึงหน้าสุดท้ายแล้วบน {current_page_url}. หยุดการดึงข้อมูล.\")\n",
        "        break # ไม่มีเนื้อหาเพิ่มเติม, หยุดลูป\n",
        "\n",
        "    # แปลงรายการ Tag object ให้เป็นสตริงแล้วเพิ่มเข้าไปใน all_scraped_content_strings\n",
        "    all_scraped_content_strings.append(\"\".join([str(element) for element in current_page_content_elements]))\n",
        "    page_num += 1\n",
        "\n",
        "# รวมเนื้อหาทั้งหมดที่ดึงมาได้จากทุกหน้าเข้าเป็นสตริงเดียวสำหรับการดำเนินการ regex ในเซลล์ถัดไป\n",
        "content = \"\".join(all_scraped_content_strings)\n",
        "print(f\"ดึงข้อมูลเสร็จสิ้นจาก {page_num - 1} หน้า\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# เลือกข้อมูลที่ต้องการสกัด\n",
        "re_titles = r'title=\"(.*?)\">'\n",
        "titles_list = re.findall(re_titles, content)\n",
        "\n",
        "re_descriptions = r'<p>(.*?)</p>'\n",
        "descriptions_list = re.findall(re_descriptions, content)"
      ],
      "metadata": {
        "id": "TFy7sihWJ2Sm"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# สร้าง DataFrame จาก lists ของ titles และ descriptions\n",
        "data = {'Title': titles_list, 'Description': descriptions_list}\n",
        "df_output = pd.DataFrame(data)\n",
        "\n",
        "# บันทึก DataFrame ลงในไฟล์ .csv\n",
        "# ใช้ encoding='utf-8-sig' เพื่อรองรับภาษาไทยได้ดีขึ้นและให้ Excel อ่านได้โดยไม่มีปัญหา\n",
        "# index=False เพื่อไม่บันทึกคอลัมน์ index ของ DataFrame ลงในไฟล์ CSV\n",
        "# sep=',' กำหนดตัวคั่นเป็น comma ซึ่งเป็นมาตรฐานสำหรับ CSV\n",
        "df_output.to_csv('output.csv', index=False, encoding='utf-8-sig', sep=',')\n",
        "\n",
        "print(\"บันทึกข้อมูลลงในไฟล์ 'output.csv' เรียบร้อยแล้วด้วย pandas\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a04a3JB2NsNW",
        "outputId": "7435c406-e394-4c5a-9ab8-d2307f3cffe4"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "บันทึกข้อมูลลงในไฟล์ 'output.csv' เรียบร้อยแล้วด้วย pandas\n"
          ]
        }
      ]
    }
  ]
}